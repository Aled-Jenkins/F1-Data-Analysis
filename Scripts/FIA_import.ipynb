{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>  FIA Importing Code </u> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>  Imports </u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> FIA Documents </u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def races_year(races_dict, year, year_string):\n",
    "    races = [race for race, race_year in races_dict.items() if year in race_year]\n",
    "    races_url = \"https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/season-\" + year_string + \"/event/\"\n",
    "    return [(race, races_url + race.replace(\" \",\"%20\") + r\"%20Grand%20Prix\") for race in races]\n",
    "\n",
    "def make_dir(folder_location):\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "def pdf_get(races_list, year, directory = '../Downloaded Data/'):\n",
    "    make_dir(directory + year)\n",
    "    for (race, url) in races_list: \n",
    "        make_dir(directory + year + \"/\" + race)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")     \n",
    "        for link in soup.select(\"a[href$='.pdf']\"):\n",
    "            filename = os.path.join(directory + year + \"/\" + race,link['href'].split('/')[-1])\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(requests.get(urljoin(url,link['href'])).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>  FIA Event Timing Documents </u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_urls(year, race_code):\n",
    "    links = []\n",
    "    url = 'https://www.fia.com/f1-archives?season=' + race_code\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")     \n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    links = [link for link in links if \"/events/fia-formula-one-world-championship\" in link]\n",
    "    links = [\"https://www.fia.com\" + link if \"https://www.fia.com\" not in link else link for link in links]\n",
    "    links = [link for link in links if year in link]\n",
    "    links = [link.replace(\"https://admin.fia.com\",\"\") for link in links]\n",
    "\n",
    "    master_dict = {}\n",
    "    key = \"season\"\n",
    "    i = 1\n",
    "    for link in links:\n",
    "        if 'https://www.fia.com/championship/events/fia-formula-one-world-championship/season-'+ year + '/' in link:\n",
    "            key = link.replace('https://www.fia.com/championship/events/fia-formula-one-world-championship/season-' + year + '/','')\n",
    "            master_dict.update({key:[link]})\n",
    "        elif i == 1:\n",
    "            key = \"season\"\n",
    "            master_dict.update({key:[link]})   \n",
    "        else:\n",
    "            my_list = master_dict[key]\n",
    "            my_list.append(link)\n",
    "            master_dict.update({key:my_list})\n",
    "        i = 0\n",
    "        \n",
    "    urls = {a:[link for link in b if \"eventtiming\" in link] for (a,b) in master_dict.items()}\n",
    "    del(urls[\"season\"])\n",
    "    if year == \"2019\":\n",
    "        urls.update({'austrian-grand-prix': ['https://www.fia.com/eventtiming-information']})\n",
    "    if year == \"2020\":    \n",
    "        urls.update({'formula-1-70th-anniversary-grand-prix': ['https://www.fia.com/events/fia-formula-one-world-championship/season-2020/formula-1-70th-anniversary-grand-prix']})\n",
    "        urls.pop('formula-1-70th-anniversary-grand')    \n",
    "    return(urls)\n",
    "\n",
    "def drop_empty_folders(directory):\n",
    "    for dirpath, dirnames, filenames in os.walk(directory, topdown=False):\n",
    "        if not dirnames and not filenames:\n",
    "            os.rmdir(dirpath)\n",
    "            \n",
    "def pdf_timings_get(urls, year, directory = '../Downloaded Data/'):\n",
    "    make_dir(directory + year)\n",
    "    for (race, url) in urls.items():\n",
    "        race = race.replace(\"-0\",\"\").replace(\"-1\",\"\").replace(\"-grand-prix\",\"\").replace(\"-\",\" \").title()\n",
    "        make_dir(directory + year + '/' + race)\n",
    "        response = requests.get(url[0])\n",
    "        soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "        for link in soup.select(\"a[href$='.pdf']\"):\n",
    "            filename = os.path.join(directory + year + '/' + race,link['href'].split('/')[-1])\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(requests.get(urljoin(url[0],link['href'])).content)\n",
    "    drop_empty_folders(directory + year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Obtaining the PDF's </u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_dict = {\n",
    "    'Abu Dhabi': [19, 20, 21],\n",
    "    'Australian': [19, 20],\n",
    "    'Austrian': [19, 20, 21],\n",
    "    'Azerbaijan': [19, 21],\n",
    "    'Bahrain': [19, 20, 21],\n",
    "    'Belgian': [19, 20, 21],\n",
    "    'Brazilian': [19, 21],\n",
    "    'British': [19, 20, 21],\n",
    "    'Dutch': [21],\n",
    "    'Canadian': [19],\n",
    "    'Chinese': [19],\n",
    "    'Eifel': [20],\n",
    "    'Emilia Romagna': [20, 21],\n",
    "    'Formula 1 70th Anniversary': [20],\n",
    "    'French': [19, 21],\n",
    "    'German': [19],\n",
    "    'Hungarian': [19, 20, 21],\n",
    "    'Italian': [19, 20, 21],\n",
    "    'Japanese': [19],\n",
    "    'Mexican': [19, 21],\n",
    "    'Monaco': [19, 21],\n",
    "    'Portuguese': [20, 21],\n",
    "    'Qatar': [21],\n",
    "    'Russian': [19, 20, 21],\n",
    "    'Sakhir': [20],\n",
    "    'Saudi Arabia': [21],\n",
    "    'Singapore': [19],\n",
    "    'Spanish': [19, 21],\n",
    "    'Styrian': [20, 21],\n",
    "    'Turkish': [20, 21],\n",
    "    'Tuscan': [20],\n",
    "    'United States': [19, 21]}\n",
    "\n",
    "races_19 = races_year(races_dict, 19, \"2019-971\")\n",
    "race_urls_2019 = race_urls(\"2019\", \"971\")\n",
    "# pdf_get(races_19, '2019')\n",
    "# pdf_timings_get(race_urls_2019, \"2019\")\n",
    "\n",
    "races_20 = races_year(races_dict, 20, \"2020-1059\")\n",
    "race_urls_2020 = race_urls(\"2020\", \"1059\")\n",
    "# pdf_get(races_20, '2018')\n",
    "# pdf_timings_get(race_urls_2020, \"2020\")\n",
    "\n",
    "races_21 = races_year(races_dict, 21, \"2021-1108\")\n",
    "race_urls_2021 = race_urls(\"2021\", \"1108\")\n",
    "# pdf_get(races_21, '2021')\n",
    "# pdf_timings_get(race_urls_2021, \"2021\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
