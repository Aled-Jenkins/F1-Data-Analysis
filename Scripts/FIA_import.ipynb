{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/season-2019-971/event/Abu%20Dhabi%20Grand%20Prix\n",
    "https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/season-2020-1059/event/Abu%20Dhabi%20Grand%20Prix\n",
    "https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/season-2021-1108/event/Abu%20Dhabi%20Grand%20Prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    'Abu Dhabi': [19, 20, 21],\n",
    "    'Australian': [19, 20],\n",
    "    'Austrian': [19, 20, 21],\n",
    "    'Azerbaijan': [19, 21],\n",
    "    'Bahrain': [19, 20, 21],\n",
    "    'Belgian': [19, 20, 21],\n",
    "    'Brazilian': [19, 21],\n",
    "    'British': [19, 20, 21],\n",
    "    'Dutch': [21],\n",
    "    'Canadian': [19],\n",
    "    'Chinese': [19],\n",
    "    'Eifel': [20],\n",
    "    'Emilia Romagna': [20, 21],\n",
    "    'Formula 1 70th Anniversary': [20],\n",
    "    'French': [19, 21],\n",
    "    'German': [19],\n",
    "    'Hungarian': [19, 20, 21],\n",
    "    'Italian': [19, 20, 21],\n",
    "    'Japanese': [19],\n",
    "    'Mexican': [19, 21],\n",
    "    'Monaco': [19, 21],\n",
    "    'Portuguese': [20, 21],\n",
    "    'Qatar': [21],\n",
    "    'Russian': [19, 20, 21],\n",
    "    'Sakhir': [20],\n",
    "    'Saudi Arabia': [21],\n",
    "    'Singapore': [19],\n",
    "    'Spanish': [19, 21],\n",
    "    'Styrian': [20, 21],\n",
    "    'Turkish': [20, 21],\n",
    "    'Tuscan': [20],\n",
    "    'United States': [19, 21]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RACE FILES\n",
    "\n",
    "def races_year(races_dict, year, year_string):\n",
    "    races = [race for race, race_year in races_dict.items() if year in race_year]\n",
    "    races_url = \"https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/season-\" + year_string + \"/event/\"\n",
    "    return [(race, races_url + race.replace(\" \",\"%20\") + r\"%20Grand%20Prix\") for race in races]\n",
    "\n",
    "def make_dir(folder_location):\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "def pdf_get(races_list, path):\n",
    "    for (race, url) in races_list: \n",
    "        make_dir(path + \"/\" + race)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")     \n",
    "        for link in soup.select(\"a[href$='.pdf']\"):\n",
    "            filename = os.path.join(path + \"/\" + race,link['href'].split('/')[-1])\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(requests.get(urljoin(url,link['href'])).content)\n",
    "\n",
    "races_19 = races_year(my_dict, 19, \"2019-971\")\n",
    "make_dir('../Downloaded Data/2019')\n",
    "pdf_get(races_19, '../Downloaded Data/2019')\n",
    "\n",
    "races_20 = races_year(my_dict, 20, \"2020-1059\")\n",
    "make_dir('../Downloaded Data/2020')\n",
    "pdf_get(races_20, '../Downloaded Data/2020')\n",
    "\n",
    "races_21 = races_year(my_dict, 21, \"2021-1108\")\n",
    "make_dir('../Downloaded Data/2021')\n",
    "pdf_get(races_21, '../Downloaded Data/2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 ALL TIMINGS\n",
    "\n",
    "links = []\n",
    "url = \"https://www.fia.com/f1-archives?season=971\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")     \n",
    "for link in soup.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "links = [link for link in links if \"/events/fia-formula-one-world-championship\" in link]\n",
    "links = [\"https://www.fia.com\" + link if \"https://www.fia.com\" not in link else link for link in links]\n",
    "links = [link for link in links if \"2019\" in link]\n",
    "links = [link.replace(\"https://admin.fia.com\",\"\") for link in links]\n",
    "\n",
    "master_dict = {}\n",
    "key = \"season\"\n",
    "i = 1\n",
    "for link in links:\n",
    "    if 'https://www.fia.com/championship/events/fia-formula-one-world-championship/season-2019/' in link:\n",
    "        key = link.replace('https://www.fia.com/championship/events/fia-formula-one-world-championship/season-2019/','')\n",
    "        master_dict.update({key:[link]})\n",
    "    elif i == 1:\n",
    "        key = \"season\"\n",
    "        master_dict.update({key:[link]})   \n",
    "    else:\n",
    "        my_list = master_dict[key]\n",
    "        my_list.append(link)\n",
    "        master_dict.update({key:my_list})\n",
    "    i = 0\n",
    "    \n",
    "urls = {a:[link for link in b if \"eventtiming\" in link] for (a,b) in master_dict.items()}\n",
    "del(urls[\"season\"])\n",
    "urls.update({'austrian-grand-prix': ['https://www.fia.com/eventtiming-information']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_location = '../Downloaded Data/2019'\n",
    "if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "for (race, url) in urls.items():\n",
    "    race = race.replace(\"-0\",\"\").replace(\"-grand-prix\",\"\").replace(\"-\",\" \").title()\n",
    "    folder_location = '../Downloaded Data/2019/'+race\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "    response = requests.get(url[0])\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "    for link in soup.select(\"a[href$='.pdf']\"):\n",
    "        filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(requests.get(urljoin(url[0],link['href'])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.fia.com/f1-archives?season=1059"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
